{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important: This notebook will only work with fastai-0.7.x. Do not try to run any fastai-1.x code from this path in the repository because it will load fastai-0.7.x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_plot_sizes(12,14,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in our data from last lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_columns(df):\n",
    "    #Drop saleElapsed (epoc time) and kcal_na (Boolean), both of which are redundant anyway\n",
    "    df = df.drop(['saleElapsed', 'kcal_na'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./\"\n",
    "\n",
    "df_raw = pd.read_feather('tmp/lunchbox-raw')\n",
    "\n",
    "df_raw = drop_irrelevant_columns(df_raw)\n",
    "\n",
    "df_trn, y_trn, nas = proc_df(df_raw, 'responsevalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(a,n): return a[:n], a[n:]\n",
    "n_valid = 40 # same as test set sample number\n",
    "n_trn = len(df_trn)-n_valid\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence based on tree variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model interpretation, there's no need to use the full dataset on each tree - using a subset will be both faster, and also provide better interpretability (since an overfit model will not provide much variance across trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.4106829813542605, 17.5657657106088, 0.9592991021976386, 0.13079721116940746]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.200368204249121, 5.078653241756125, 0.9619258384262565, 0.9273418958414028]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "#Taking the whole Training set including the Validation set separated earlier\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 21.859490215922236, 1.0, -0.34606799535194877]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, bootstrap=False)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Overfitting exmaple, using the entire Training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.3725749761260015, 18.29259071318221, 0.9597815531371131, 0.057378428817916105, 0.7056476016264986]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As known, 1/3 of samples are NOT used with bootstrap=True (default), so oob_score use that one-third for Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.105885180081208, 5.443239786652799, 0.9630773678141703, 0.9165354854044157, 0.775880772133376]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "#Using the whole Training set with oob_score\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.920713743716203, 5.08381008693283, 0.9652828939611495, 0.9271942674037819, 0.7723377887407956]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=80, n_jobs=-1, oob_score=True)\n",
    "#Using the whole Training set with oob_score\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.930399569609361, 5.893963042573495, 0.9651692120944404, 0.9021407786267904, 0.7699606942219437]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=120, n_jobs=-1, oob_score=True)\n",
    "#Using the whole Training set with oob_score\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.752744775764855, 5.472464709799415, 0.9672247811015486, 0.9156368322828268, 0.7840647951686446]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=100, n_jobs=-1, oob_score=True)\n",
    "#Using the whole Training set with oob_score\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.845486690155448, 5.260203781904319, 0.966159502469809, 0.9220543150754308, 0.7780874961569884]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=90, n_jobs=-1, oob_score=True)\n",
    "#Using the whole Training set with oob_score\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.6060757943366575, 5.275082741899344, 0.9688747163577269, 0.921612738662955, 0.7825761591524527]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=110, n_jobs=-1, oob_score=True)\n",
    "#Using the whole Training set with oob_score\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out this, as our training test is with too few samples!\n",
    "# Set to 167 (207 train samples (November 18, 2013 to September 30, 2014) - 40 test sample)\n",
    "set_rf_samples(167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now randomise the same 167 samples from Training set, which is the same size in fact!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.282300962961068, 18.021954926561104, 0.960912952758559, 0.08506395471671524, 0.7010578921345584]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "# Way too few samples we have, so omit min_samples_leaf=3, max_features=0.5\n",
    "#m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 21.79760630493633, 1.0, -0.3384573840628191]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, bootstrap=False)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.909530110864836, 16.766817117449573, 0.9213850780866538, 0.2080674495580831, 0.7257816410708554]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "# Way too few samples we have, so omit min_samples_leaf=3, max_features=0.5\n",
    "#m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 21.986642251603584, 1.0, -0.3617731258142891]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1, bootstrap=False)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Although using randomly chosen samples of 110, fewer than Training set (167), it still use the ENTIRE Training set samples due to bootstrap=False!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Only Two Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.386164433395923, 9.346113113610919, 0.9127487406785195, 0.7539354756648896, 0.7650441926753309]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=190, n_jobs=-1, oob_score=True)\n",
    "#m = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "#%time m.fit(df_ext, y_train)\n",
    "m.fit(df_trn, y_trn)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_feather('tmp/lunchbox-test')\n",
    "#convert every feature values to numeric\n",
    "df_test, _, _ = proc_df(df_test)\n",
    "\n",
    "#Number of features of the model must match the input\n",
    "df_test = drop_irrelevant_columns(df_test)\n",
    "\n",
    "predictions = m.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61.10526,  54.42632,  53.83158,  57.77368,  56.51053,  54.79474,  53.93684,  92.02105,  53.47895,\n",
       "       116.91579,  68.6    ,  58.45263,  59.78947,  61.52105, 116.98947, 112.19474,  83.54211,  62.02105,\n",
       "        66.4    ,  62.62105,  64.16842,  67.72632,  68.64211, 119.53684,  64.78421,  89.56842,  65.94737,\n",
       "       119.58421, 117.37895,  66.92632, 107.28947, 120.61579, 117.41053, 115.20526, 109.18421, 111.98421,\n",
       "       138.85263, 137.93158, 134.5    , 130.57368])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n",
      "Parameters currently in use:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REFERENCE: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "#Turn off Autosave\n",
    "%autosave 0\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print (\"Parameters currently in use:\\n\")\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [40, 45, 51, 57, 63, 68, 74, 80, 86, 92, 97, 103, 109, 115, 121, 126, 132, 138, 144, 150], 'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 40, stop = 150, num = 20)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "#min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "#min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the grid\n",
    "grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   25.4s finished\n",
      "/home/to/miniconda3/envs/signate/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'n_estimators': [40, 45, 51, 57, 63, 68, 74, 80, 86, 92, 97, 103, 109, 115, 121, 126, 132, 138, 144, 150], 'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "#cv = 5 as test sample (40) is roughly one-fifth of training test sample (207 train samples (November 18, 2013 to September 30, 2014)) \n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = grid, cv = 5, verbose=2, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_grid.fit(df_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True, 'max_features': 'auto', 'n_estimators': 121}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=121, n_jobs=-1)\n",
    "m.fit(df_trn, y_trn)\n",
    "\n",
    "predictions = m.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 61.30579,  55.23967,  57.91736,  60.6281 ,  57.17355,  55.42975,  54.73554,  91.30579,  54.36364,\n",
       "       118.17355,  70.93388,  59.71074,  58.52066,  58.80992, 117.1157 , 108.68595,  81.12397,  60.54545,\n",
       "        65.32231,  61.04132,  60.61983,  66.66116,  71.91736, 118.41322,  67.47934,  88.95041,  65.89256,\n",
       "       116.45455, 114.77686,  70.43802, 107.33058, 123.47107, 118.36364, 115.77686, 108.67769, 111.92562,\n",
       "       137.03306, 136.35537, 130.42975, 125.91736])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
